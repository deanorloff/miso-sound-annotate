# Annotation guide 

##   Initial search

###  Order of search terms

-   Divide into umbrella categories (e.g. "eating", "drinking")

-   Sort individual search terms by number of times cited

-   Sort umbrella categories by number of times cited in total

###  Search procedure

-   Start with the search term that is most cited, within the
umbrella category that is most cited

-   Sort sounds as "automatic by relevance" on Freesound search
engine

-   Download each sound, and by reading the user-provided
metadata and listening to the sound, add the following to a
spreadsheet: url, search\_term, search\_term\_present,
other\_search\_term\_present

-   For all sounds with search\_term\_present = Yes (regardless
of duration), label in Audacity (see below)

-   Once there are 5 unique files containing at least 15 seconds
of continuous sounds in the target umbrella category, move
onto the next most cited umbrella category. Continuous =
offset and onset of subsequent sounds separated by no more
than 2 seconds

-   Otherwise continue until there are no more sounds for this
search term, and then move onto the next search term in the
category until all search terms in the category have been
exhausted

## Labeling relevant sounds

-   Label the onset and offset of each sound in Audacity with a
sufficiently detailed descriptive label, exporting the
annotations as a text file that is named identically to the
audio file name.

-   The descriptive label may not match the search terms exactly and
may vary in level of detail (e.g. "woman sniffling" could be
provided in user metadata but if not, it may not be possible to
determine a more detailed label than "sniffling"). For each new
descriptor, add it to a spreadsheet containing descriptive
labels and umbrella categories.

-   If there is at least 250â€†ms between instances of a sound, label
the onset & offset separately
